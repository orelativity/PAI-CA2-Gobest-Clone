{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cd09aa7",
   "metadata": {},
   "source": [
    "# **Build Feature Engineering Script for GUI Inference (XGBoost)**\n",
    "\n",
    "This notebook creates a reusable `feature_engineer.py` module for our Tkinter GUI.\n",
    "\n",
    "Goal:\n",
    "- Accept raw inputs (sensor_data, driver_data, safety_labels)\n",
    "- Engineer trip-level features (1 row per bookingID)\n",
    "- Match the exact feature schema used by the XGBoost model (`xgboost_feature_cols.json`)\n",
    "- Support both batch inference (all trips) and single-trip inference (select bookingID)\n",
    "\n",
    "Important:\n",
    "- The GUI will call this script before sending features to the saved scaler + XGBoost model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "565ce35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote: C:\\PAI-GoBest-Project\\Sprint 2\\Tkinter\\feature_engineer.py\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# this cell writes feature_engineer.py\n",
    "# save into: Sprint 2/Tkinter/feature_engineer.py\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "FEATURE_ENGINEER_PY = r'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================\n",
    "# thresholds follow the Feature_Engineering notebook\n",
    "# ============================================================\n",
    "THRESHOLDS = {\n",
    "    # acceleration thresholds (m/s^2)\n",
    "    \"harsh_acceleration\": 4.5,\n",
    "    \"harsh_braking\": -5.5,\n",
    "    \"max_acceleration_cap\": 10.0,\n",
    "    \"max_acceleration_z_cap\": 12.0,\n",
    "\n",
    "    # speed thresholds (m/s)\n",
    "    \"speeding_limit\": 33.3,      # 120 km/h\n",
    "    \"high_speed\": 25.0,\n",
    "    \"max_speed_cap\": 50.0,\n",
    "\n",
    "    # gyro thresholds (rad/s)\n",
    "    \"sharp_turn\": 2.0,\n",
    "    \"gyro_stability\": 0.5,\n",
    "    \"gyro_peak_height\": 1.5,\n",
    "    \"max_gyro_cap\": 4.0,\n",
    "\n",
    "    # gps accuracy threshold (m)\n",
    "    \"low_gps_accuracy\": 30.0,\n",
    "\n",
    "    # rolling windows (rows, since sensor is per-second mostly)\n",
    "    \"rolling_window_5s\": 5,\n",
    "    \"rolling_window_10s\": 10,\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# helpers\n",
    "# ============================================================\n",
    "def _coerce_bool_label(x):\n",
    "    # supports true/false, 0/1, yes/no\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    s = str(x).strip().lower()\n",
    "    if s in {\"true\", \"1\", \"yes\", \"y\"}:\n",
    "        return 1\n",
    "    if s in {\"false\", \"0\", \"no\", \"n\"}:\n",
    "        return 0\n",
    "    try:\n",
    "        return int(float(s))\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def _clip_sensor_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # speed\n",
    "    if \"speed\" in df.columns:\n",
    "        df[\"speed\"] = pd.to_numeric(df[\"speed\"], errors=\"coerce\").clip(\n",
    "            THRESHOLDS[\"speed_lower\"] if \"speed_lower\" in THRESHOLDS else 0,\n",
    "            THRESHOLDS[\"max_speed_cap\"],\n",
    "        )\n",
    "\n",
    "    # accel\n",
    "    for c in [\"acceleration_x\", \"acceleration_y\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").clip(\n",
    "                -THRESHOLDS[\"max_acceleration_cap\"],\n",
    "                THRESHOLDS[\"max_acceleration_cap\"],\n",
    "            )\n",
    "\n",
    "    if \"acceleration_z\" in df.columns:\n",
    "        df[\"acceleration_z\"] = pd.to_numeric(df[\"acceleration_z\"], errors=\"coerce\").clip(\n",
    "            -THRESHOLDS[\"max_acceleration_z_cap\"],\n",
    "            THRESHOLDS[\"max_acceleration_z_cap\"],\n",
    "        )\n",
    "\n",
    "    # gyro\n",
    "    for c in [\"gyro_x\", \"gyro_y\", \"gyro_z\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").clip(\n",
    "                -THRESHOLDS[\"max_gyro_cap\"],\n",
    "                THRESHOLDS[\"max_gyro_cap\"],\n",
    "            )\n",
    "\n",
    "    # accuracy\n",
    "    if \"accuracy\" in df.columns:\n",
    "        df[\"accuracy\"] = pd.to_numeric(df[\"accuracy\"], errors=\"coerce\")\n",
    "\n",
    "    # bearing and second\n",
    "    if \"bearing\" in df.columns:\n",
    "        df[\"bearing\"] = pd.to_numeric(df[\"bearing\"], errors=\"coerce\")\n",
    "    if \"second\" in df.columns:\n",
    "        df[\"second\"] = pd.to_numeric(df[\"second\"], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def _rms(x: np.ndarray) -> float:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if x.size == 0:\n",
    "        return 0.0\n",
    "    return float(np.sqrt(np.mean(x * x)))\n",
    "\n",
    "\n",
    "def _skewness(x: np.ndarray) -> float:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if x.size < 3:\n",
    "        return 0.0\n",
    "    m = np.mean(x)\n",
    "    s = np.std(x) + 1e-12\n",
    "    return float(np.mean(((x - m) / s) ** 3))\n",
    "\n",
    "\n",
    "def _kurtosis(x: np.ndarray) -> float:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if x.size < 4:\n",
    "        return 0.0\n",
    "    m = np.mean(x)\n",
    "    s = np.std(x) + 1e-12\n",
    "    return float(np.mean(((x - m) / s) ** 4) - 3.0)\n",
    "\n",
    "\n",
    "def _mean_change(x: np.ndarray) -> float:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if x.size < 2:\n",
    "        return 0.0\n",
    "    return float(np.mean(np.abs(np.diff(x))))\n",
    "\n",
    "\n",
    "def _max_change(x: np.ndarray) -> float:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if x.size < 2:\n",
    "        return 0.0\n",
    "    return float(np.max(np.abs(np.diff(x))))\n",
    "\n",
    "\n",
    "def _count_above_mean(x: np.ndarray) -> int:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if x.size == 0:\n",
    "        return 0\n",
    "    m = np.mean(x)\n",
    "    return int(np.sum(x > m))\n",
    "\n",
    "\n",
    "def _count_below_mean(x: np.ndarray) -> int:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if x.size == 0:\n",
    "        return 0\n",
    "    m = np.mean(x)\n",
    "    return int(np.sum(x < m))\n",
    "\n",
    "\n",
    "def _ts_stats(prefix: str, x: np.ndarray, include_median: bool, include_abs_max: bool, include_rms: bool) -> dict:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if x.size == 0:\n",
    "        x = np.asarray([0.0], dtype=float)\n",
    "\n",
    "    q25 = float(np.quantile(x, 0.25))\n",
    "    q75 = float(np.quantile(x, 0.75))\n",
    "    out = {\n",
    "        f\"{prefix}__mean\": float(np.mean(x)),\n",
    "        f\"{prefix}__std\": float(np.std(x)),\n",
    "        f\"{prefix}__min\": float(np.min(x)),\n",
    "        f\"{prefix}__max\": float(np.max(x)),\n",
    "        f\"{prefix}__range\": float(np.max(x) - np.min(x)),\n",
    "        f\"{prefix}__q25\": q25,\n",
    "        f\"{prefix}__q75\": q75,\n",
    "        f\"{prefix}__iqr\": float(q75 - q25),\n",
    "        f\"{prefix}__skewness\": _skewness(x),\n",
    "        f\"{prefix}__kurtosis\": _kurtosis(x),\n",
    "        f\"{prefix}__count_above_mean\": _count_above_mean(x),\n",
    "        f\"{prefix}__count_below_mean\": _count_below_mean(x),\n",
    "        f\"{prefix}__mean_change\": _mean_change(x),\n",
    "        f\"{prefix}__max_change\": _max_change(x),\n",
    "    }\n",
    "\n",
    "    if include_median:\n",
    "        out[f\"{prefix}__median\"] = float(np.median(x))\n",
    "    if include_abs_max:\n",
    "        out[f\"{prefix}__abs_max\"] = float(np.max(np.abs(x)))\n",
    "    if include_rms:\n",
    "        out[f\"{prefix}__rms\"] = _rms(x)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def _build_vehicle_key(driver_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # replicate ETL idea: build a stable surrogate key from car_make + car_model_year\n",
    "    df = driver_df.copy()\n",
    "\n",
    "    if \"car_make\" not in df.columns or \"car_model_year\" not in df.columns:\n",
    "        df[\"vehicle_key\"] = 0\n",
    "        return df\n",
    "\n",
    "    df[\"car_make\"] = df[\"car_make\"].astype(str)\n",
    "    df[\"car_model_year\"] = pd.to_numeric(df[\"car_model_year\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    # stable mapping across the provided file\n",
    "    unique = (\n",
    "        df[[\"car_make\", \"car_model_year\"]]\n",
    "        .drop_duplicates()\n",
    "        .assign(vehicle_id_natural=lambda d: d[\"car_make\"] + \"_\" + d[\"car_model_year\"].astype(str))\n",
    "        .sort_values(\"vehicle_id_natural\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    unique[\"vehicle_key\"] = np.arange(1, len(unique) + 1)\n",
    "\n",
    "    df = df.merge(unique[[\"car_make\", \"car_model_year\", \"vehicle_key\"]], on=[\"car_make\", \"car_model_year\"], how=\"left\")\n",
    "    df[\"vehicle_key\"] = df[\"vehicle_key\"].fillna(0).astype(float)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def engineer_features_from_raw_tables(\n",
    "    sensor_df: pd.DataFrame,\n",
    "    driver_df: pd.DataFrame,\n",
    "    safety_df: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    main public API for GUI\n",
    "\n",
    "    inputs:\n",
    "    - sensor_df: raw sensor table (must include bookingID)\n",
    "    - driver_df: raw driver table (must include id)\n",
    "    - safety_df: raw safety labels (must include bookingID and driver_id). label optional for inference.\n",
    "\n",
    "    output:\n",
    "    - engineered_df: one row per bookingID containing engineered features\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------------\n",
    "    # normalize column names\n",
    "    # -------------------------------\n",
    "    sensor_df = sensor_df.copy()\n",
    "    driver_df = driver_df.copy()\n",
    "    safety_df = safety_df.copy()\n",
    "\n",
    "    sensor_df.columns = [c.strip() for c in sensor_df.columns]\n",
    "    driver_df.columns = [c.strip() for c in driver_df.columns]\n",
    "    safety_df.columns = [c.strip() for c in safety_df.columns]\n",
    "\n",
    "    # required columns\n",
    "    if \"bookingID\" not in sensor_df.columns:\n",
    "        raise ValueError(\"sensor_data must include bookingID column\")\n",
    "\n",
    "    # ensure safety contains mapping\n",
    "    if \"bookingID\" not in safety_df.columns or \"driver_id\" not in safety_df.columns:\n",
    "        raise ValueError(\"safety_labels must include bookingID and driver_id columns\")\n",
    "\n",
    "    # coerce label\n",
    "    if \"label\" in safety_df.columns:\n",
    "        safety_df[\"label\"] = safety_df[\"label\"].apply(_coerce_bool_label)\n",
    "\n",
    "    # driver id key\n",
    "    if \"id\" not in driver_df.columns:\n",
    "        raise ValueError(\"driver_data must include id column\")\n",
    "\n",
    "    # build vehicle_key\n",
    "    driver_df = _build_vehicle_key(driver_df)\n",
    "\n",
    "    # -------------------------------\n",
    "    # merge driver info into safety\n",
    "    # -------------------------------\n",
    "    safety_driver = safety_df.merge(\n",
    "        driver_df,\n",
    "        left_on=\"driver_id\",\n",
    "        right_on=\"id\",\n",
    "        how=\"left\",\n",
    "        suffixes=(\"\", \"_driver\"),\n",
    "    )\n",
    "\n",
    "    # -------------------------------\n",
    "    # sensor preprocessing\n",
    "    # -------------------------------\n",
    "    sensor_df = _clip_sensor_values(sensor_df)\n",
    "\n",
    "    # fill missing numeric with 0 (gui assumption: no nulls, but we still guard)\n",
    "    for c in [\"accuracy\",\"bearing\",\"second\",\"speed\",\"acceleration_x\",\"acceleration_y\",\"acceleration_z\",\"gyro_x\",\"gyro_y\",\"gyro_z\"]:\n",
    "        if c in sensor_df.columns:\n",
    "            sensor_df[c] = pd.to_numeric(sensor_df[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    # sort within trip\n",
    "    if \"second\" in sensor_df.columns:\n",
    "        sensor_df = sensor_df.sort_values([\"bookingID\", \"second\"])\n",
    "\n",
    "    # -------------------------------\n",
    "    # per-trip aggregation\n",
    "    # -------------------------------\n",
    "    rows = []\n",
    "    for bid, g in sensor_df.groupby(\"bookingID\"):\n",
    "        g = g.copy()\n",
    "        n = len(g)\n",
    "\n",
    "        # time and distance\n",
    "        if \"second\" in g.columns and n > 0:\n",
    "            tmin = float(g[\"second\"].min())\n",
    "            tmax = float(g[\"second\"].max())\n",
    "            trip_duration_sec = float(max(0.0, tmax - tmin))\n",
    "        else:\n",
    "            trip_duration_sec = float(n)\n",
    "\n",
    "        # distance estimate: assume ~1 second step\n",
    "        # distance(m) ~ sum(speed) * 1\n",
    "        total_distance_km = float(g[\"speed\"].sum() / 1000.0)\n",
    "\n",
    "        avg_gps_accuracy = float(g[\"accuracy\"].mean()) if \"accuracy\" in g.columns else 0.0\n",
    "\n",
    "        # accel magnitude\n",
    "        ax = g[\"acceleration_x\"].to_numpy()\n",
    "        ay = g[\"acceleration_y\"].to_numpy()\n",
    "        az = g[\"acceleration_z\"].to_numpy()\n",
    "        accel_mag = np.sqrt(ax*ax + ay*ay + az*az)\n",
    "\n",
    "        # gyro magnitude\n",
    "        gx = g[\"gyro_x\"].to_numpy()\n",
    "        gy = g[\"gyro_y\"].to_numpy()\n",
    "        gz = g[\"gyro_z\"].to_numpy()\n",
    "        gyro_mag = np.sqrt(gx*gx + gy*gy + gz*gz)\n",
    "\n",
    "        # events\n",
    "        harsh_acceleration_count = int(np.sum(g[\"acceleration_x\"] > THRESHOLDS[\"harsh_acceleration\"]))\n",
    "        harsh_braking_count = int(np.sum(g[\"acceleration_x\"] < THRESHOLDS[\"harsh_braking\"]))\n",
    "        sharp_turn_count = int(np.sum(np.abs(g[\"gyro_z\"]) > THRESHOLDS[\"sharp_turn\"]))\n",
    "        speeding_event_count = int(np.sum(g[\"speed\"] > THRESHOLDS[\"speeding_limit\"]))\n",
    "\n",
    "        # phone distraction proxy: poor gps quality count\n",
    "        phone_distraction_count = int(np.sum(g[\"accuracy\"] > THRESHOLDS[\"low_gps_accuracy\"])) if \"accuracy\" in g.columns else 0\n",
    "\n",
    "        avg_acceleration_magnitude = float(np.mean(accel_mag)) if accel_mag.size else 0.0\n",
    "        max_acceleration_magnitude = float(np.max(accel_mag)) if accel_mag.size else 0.0\n",
    "\n",
    "        # rolling features\n",
    "        w5 = THRESHOLDS[\"rolling_window_5s\"]\n",
    "        w10 = THRESHOLDS[\"rolling_window_10s\"]\n",
    "\n",
    "        speed_rolling_std_5s = float(pd.Series(g[\"speed\"]).rolling(w5, min_periods=1).std().mean())\n",
    "        accel_x_rolling_max_10s = float(pd.Series(g[\"acceleration_x\"]).rolling(w10, min_periods=1).max().mean())\n",
    "        gyro_z_rolling_range_5s = float(\n",
    "            (pd.Series(g[\"gyro_z\"]).rolling(w5, min_periods=1).max()\n",
    "             - pd.Series(g[\"gyro_z\"]).rolling(w5, min_periods=1).min()).mean()\n",
    "        )\n",
    "        accel_magnitude_rolling_mean_5s = float(pd.Series(accel_mag).rolling(w5, min_periods=1).mean().mean())\n",
    "\n",
    "        # speed change rate\n",
    "        speed_change_rate = float(_mean_change(g[\"speed\"].to_numpy()))\n",
    "\n",
    "        # phase features\n",
    "        third = max(1, n // 3)\n",
    "        phase1 = g.iloc[:third]\n",
    "        phase2 = g.iloc[third:2*third]\n",
    "        phase3 = g.iloc[2*third:]\n",
    "\n",
    "        accel_x_first_third_mean = float(phase1[\"acceleration_x\"].mean()) if len(phase1) else 0.0\n",
    "        speed_last_third_std = float(phase3[\"speed\"].std()) if len(phase3) else 0.0\n",
    "\n",
    "        total_harsh_braking = int(np.sum(g[\"acceleration_x\"] < THRESHOLDS[\"harsh_braking\"]))\n",
    "        middle_harsh_braking = int(np.sum(phase2[\"acceleration_x\"] < THRESHOLDS[\"harsh_braking\"])) if len(phase2) else 0\n",
    "        harsh_braking_middle_third_ratio = float(middle_harsh_braking / max(1, total_harsh_braking))\n",
    "\n",
    "        # jerk features (derivative of acceleration)\n",
    "        jerk_x = np.diff(g[\"acceleration_x\"].to_numpy(), prepend=g[\"acceleration_x\"].iloc[0])\n",
    "        jerk_y = np.diff(g[\"acceleration_y\"].to_numpy(), prepend=g[\"acceleration_y\"].iloc[0])\n",
    "        jerk_z = np.diff(g[\"acceleration_z\"].to_numpy(), prepend=g[\"acceleration_z\"].iloc[0])\n",
    "        jerk_mag = np.sqrt(jerk_x*jerk_x + jerk_y*jerk_y + jerk_z*jerk_z)\n",
    "\n",
    "        jerk_x_mean = float(np.mean(jerk_x))\n",
    "        jerk_y_max = float(np.max(jerk_y)) if jerk_y.size else 0.0\n",
    "        jerk_z_std = float(np.std(jerk_z))\n",
    "        jerk_magnitude_std = float(np.std(jerk_mag))\n",
    "\n",
    "        # gyro features\n",
    "        gyro_total_rotation = float(np.sum(np.abs(gx)) + np.sum(np.abs(gy)) + np.sum(np.abs(gz)))\n",
    "        gyro_magnitude_max = float(np.max(gyro_mag)) if gyro_mag.size else 0.0\n",
    "        gyro_stability_ratio = float(np.mean(\n",
    "            (np.abs(gx) < THRESHOLDS[\"gyro_stability\"]) &\n",
    "            (np.abs(gy) < THRESHOLDS[\"gyro_stability\"]) &\n",
    "            (np.abs(gz) < THRESHOLDS[\"gyro_stability\"])\n",
    "        ))\n",
    "\n",
    "        # simple peak count proxy\n",
    "        gyro_z_peak_count = int(np.sum(np.abs(gz) > THRESHOLDS[\"gyro_peak_height\"]))\n",
    "\n",
    "        # interaction features\n",
    "        speed_accel_product = float(np.mean(g[\"speed\"].to_numpy() * accel_mag)) if n else 0.0\n",
    "        harsh_decel_at_high_speed_count = int(np.sum(\n",
    "            (g[\"acceleration_x\"] < THRESHOLDS[\"harsh_braking\"]) &\n",
    "            (g[\"speed\"] > THRESHOLDS[\"high_speed\"])\n",
    "        ))\n",
    "        accel_variance_normalized_by_speed = float(np.var(accel_mag) / (np.mean(g[\"speed\"]) + 1e-6))\n",
    "\n",
    "        # core row\n",
    "        row = {\n",
    "            \"bookingID\": bid,\n",
    "            \"trip_duration_sec\": trip_duration_sec,\n",
    "            \"total_distance_km\": total_distance_km,\n",
    "            \"avg_gps_accuracy\": avg_gps_accuracy,\n",
    "            \"harsh_acceleration_count\": harsh_acceleration_count,\n",
    "            \"harsh_braking_count\": harsh_braking_count,\n",
    "            \"sharp_turn_count\": sharp_turn_count,\n",
    "            \"speeding_event_count\": speeding_event_count,\n",
    "            \"phone_distraction_count\": phone_distraction_count,\n",
    "            \"avg_acceleration_magnitude\": avg_acceleration_magnitude,\n",
    "            \"max_acceleration_magnitude\": max_acceleration_magnitude,\n",
    "            \"speed_rolling_std_5s\": speed_rolling_std_5s,\n",
    "            \"accel_x_rolling_max_10s\": accel_x_rolling_max_10s,\n",
    "            \"gyro_z_rolling_range_5s\": gyro_z_rolling_range_5s,\n",
    "            \"speed_change_rate\": speed_change_rate,\n",
    "            \"accel_magnitude_rolling_mean_5s\": accel_magnitude_rolling_mean_5s,\n",
    "            \"accel_x_first_third_mean\": accel_x_first_third_mean,\n",
    "            \"speed_last_third_std\": speed_last_third_std,\n",
    "            \"harsh_braking_middle_third_ratio\": harsh_braking_middle_third_ratio,\n",
    "            \"jerk_x_mean\": jerk_x_mean,\n",
    "            \"jerk_y_max\": jerk_y_max,\n",
    "            \"jerk_z_std\": jerk_z_std,\n",
    "            \"jerk_magnitude_std\": jerk_magnitude_std,\n",
    "            \"gyro_total_rotation\": gyro_total_rotation,\n",
    "            \"gyro_z_peak_count\": gyro_z_peak_count,\n",
    "            \"gyro_stability_ratio\": gyro_stability_ratio,\n",
    "            \"gyro_magnitude_max\": gyro_magnitude_max,\n",
    "            \"speed_accel_product\": speed_accel_product,\n",
    "            \"harsh_decel_at_high_speed_count\": harsh_decel_at_high_speed_count,\n",
    "            \"accel_variance_normalized_by_speed\": accel_variance_normalized_by_speed,\n",
    "        }\n",
    "\n",
    "        # tsfresh-like stats (only the ones used by model feature list)\n",
    "        # speed\n",
    "        row.update(_ts_stats(\"speed\", g[\"speed\"].to_numpy(), include_median=True, include_abs_max=False, include_rms=False))\n",
    "        # accel\n",
    "        row.update(_ts_stats(\"acceleration_x\", g[\"acceleration_x\"].to_numpy(), include_median=False, include_abs_max=True, include_rms=True))\n",
    "        row.update(_ts_stats(\"acceleration_y\", g[\"acceleration_y\"].to_numpy(), include_median=False, include_abs_max=True, include_rms=True))\n",
    "        row.update(_ts_stats(\"acceleration_z\", g[\"acceleration_z\"].to_numpy(), include_median=False, include_abs_max=True, include_rms=True))\n",
    "        # gyro (gyro_x/y/z include median, abs_max. gyro_y and gyro_z include rms in feature list, so do rms for all gyro to be safe)\n",
    "        row.update(_ts_stats(\"gyro_x\", g[\"gyro_x\"].to_numpy(), include_median=True, include_abs_max=True, include_rms=False))\n",
    "        row.update(_ts_stats(\"gyro_y\", g[\"gyro_y\"].to_numpy(), include_median=True, include_abs_max=True, include_rms=True))\n",
    "        row.update(_ts_stats(\"gyro_z\", g[\"gyro_z\"].to_numpy(), include_median=True, include_abs_max=True, include_rms=True))\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    engineered = pd.DataFrame(rows)\n",
    "\n",
    "    # -------------------------------\n",
    "    # attach safety + driver columns needed by model\n",
    "    # -------------------------------\n",
    "    # keep only minimal driver columns needed for history features and vehicle_key\n",
    "    keep_driver_cols = [\"bookingID\", \"driver_id\", \"label\", \"vehicle_key\"]\n",
    "\n",
    "    if \"no_of_years_driving_exp\" in safety_driver.columns:\n",
    "        keep_driver_cols.append(\"no_of_years_driving_exp\")\n",
    "    if \"rating\" in safety_driver.columns:\n",
    "        keep_driver_cols.append(\"rating\")\n",
    "    if \"date_of_birth\" in safety_driver.columns:\n",
    "        keep_driver_cols.append(\"date_of_birth\")\n",
    "\n",
    "    trip_meta = safety_driver[keep_driver_cols].drop_duplicates(\"bookingID\")\n",
    "    engineered = engineered.merge(trip_meta, on=\"bookingID\", how=\"left\")\n",
    "\n",
    "    # if vehicle_key missing, fill\n",
    "    if \"vehicle_key\" not in engineered.columns:\n",
    "        engineered[\"vehicle_key\"] = 0.0\n",
    "    engineered[\"vehicle_key\"] = pd.to_numeric(engineered[\"vehicle_key\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    # -------------------------------\n",
    "    # driver history features\n",
    "    # computed using the uploaded dataset (not magic)\n",
    "    # -------------------------------\n",
    "    if \"driver_id\" in engineered.columns:\n",
    "        # driver average harsh accel from this dataset\n",
    "        engineered[\"driver_avg_harsh_accel_historical\"] = (\n",
    "            engineered.groupby(\"driver_id\")[\"harsh_acceleration_count\"].transform(\"mean\").fillna(0.0)\n",
    "        )\n",
    "\n",
    "        if \"label\" in engineered.columns:\n",
    "            engineered[\"driver_dangerous_trip_rate_historical\"] = (\n",
    "                engineered.groupby(\"driver_id\")[\"label\"].transform(\"mean\").fillna(0.0)\n",
    "            )\n",
    "        else:\n",
    "            engineered[\"driver_dangerous_trip_rate_historical\"] = 0.0\n",
    "\n",
    "        # deviation from driver norm (use avg speed mean)\n",
    "        driver_speed_mean = engineered.groupby(\"driver_id\")[\"speed__mean\"].transform(\"mean\")\n",
    "        driver_speed_std = engineered.groupby(\"driver_id\")[\"speed__mean\"].transform(\"std\").fillna(0.0)\n",
    "        engineered[\"trip_deviation_from_driver_norm\"] = (\n",
    "            (engineered[\"speed__mean\"] - driver_speed_mean).abs() / (driver_speed_std + 1e-6)\n",
    "        ).fillna(0.0)\n",
    "    else:\n",
    "        engineered[\"driver_avg_harsh_accel_historical\"] = 0.0\n",
    "        engineered[\"driver_dangerous_trip_rate_historical\"] = 0.0\n",
    "        engineered[\"trip_deviation_from_driver_norm\"] = 0.0\n",
    "\n",
    "    return engineered\n",
    "\n",
    "\n",
    "def load_raw_csvs(sensor_path: str, driver_path: str, safety_path: str):\n",
    "    sensor_df = pd.read_csv(sensor_path)\n",
    "    driver_df = pd.read_csv(driver_path)\n",
    "    safety_df = pd.read_csv(safety_path)\n",
    "    return sensor_df, driver_df, safety_df\n",
    "'''\n",
    "\n",
    "# change this to your project path when you paste into your notebook\n",
    "out_path = Path.cwd() / \"feature_engineer.py\"\n",
    "out_path.write_text(FEATURE_ENGINEER_PY, encoding=\"utf-8\")\n",
    "print(\"wrote:\", out_path.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0b7a14",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50c1dd2",
   "metadata": {},
   "source": [
    "# **Quick Test: Build Engineered Features from Raw 3 CSVs**\n",
    "\n",
    "This section validates that the feature engineering script produces 1 row per bookingID\n",
    "and includes the key engineered columns needed by the XGBoost model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "729ec1aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sensor_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m DRIVER_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m SAFETY_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msafety_labels.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 10\u001b[0m sensor_df, driver_df, safety_df \u001b[38;5;241m=\u001b[39m \u001b[43mload_raw_csvs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSENSOR_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDRIVER_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSAFETY_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m engineered \u001b[38;5;241m=\u001b[39m engineer_features_from_raw_tables(sensor_df, driver_df, safety_df)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(engineered\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\PAI-GoBest-Project\\Sprint 2\\Tkinter\\feature_engineer.py:493\u001b[0m, in \u001b[0;36mload_raw_csvs\u001b[1;34m(sensor_path, driver_path, safety_path)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_raw_csvs\u001b[39m(sensor_path: \u001b[38;5;28mstr\u001b[39m, driver_path: \u001b[38;5;28mstr\u001b[39m, safety_path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 493\u001b[0m     sensor_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43msensor_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    494\u001b[0m     driver_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(driver_path)\n\u001b[0;32m    495\u001b[0m     safety_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(safety_path)\n",
      "File \u001b[1;32mc:\\Users\\ezinn\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ezinn\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\ezinn\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ezinn\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\ezinn\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sensor_data.csv'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from feature_engineer import load_raw_csvs, engineer_features_from_raw_tables\n",
    "\n",
    "# update these paths to your raw csvs\n",
    "SENSOR_PATH = \"sensor_data.csv\"\n",
    "DRIVER_PATH = \"driver_data.csv\"\n",
    "SAFETY_PATH = \"safety_labels.csv\"\n",
    "\n",
    "sensor_df, driver_df, safety_df = load_raw_csvs(SENSOR_PATH, DRIVER_PATH, SAFETY_PATH)\n",
    "\n",
    "engineered = engineer_features_from_raw_tables(sensor_df, driver_df, safety_df)\n",
    "print(engineered.shape)\n",
    "print(engineered.head(3))\n",
    "\n",
    "# check feature coverage using your model feature list\n",
    "feature_cols = json.loads(open(\"xgboost_feature_cols.json\",\"r\",encoding=\"utf-8\").read())\n",
    "missing = [c for c in feature_cols if c not in engineered.columns]\n",
    "extra = [c for c in engineered.columns if c not in ([\"bookingID\",\"driver_id\",\"label\"] + feature_cols)]\n",
    "\n",
    "print(\"missing required:\", len(missing))\n",
    "print(missing[:20])\n",
    "print(\"extra cols:\", len(extra))\n",
    "print(extra[:20])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
